\chapter{Background and Motivation}
\label{chap:background}

% Introduce benchmarks
Benchmark harness sets up an environment, executes benchmark code in multiple iterations, and then presents some score.
The score can be iteration time, throughput, number of processed requests, etcetera.
When a benchmark is a small piece of code, also referred to as performance unit tests by~\citet{horky2015unit}, it is usually written in some microbenchmark framework.
Microbenchmark execution in a public cloud has been thoroughly examined by~\citet{laaber2019software}.
If the benchmark code gets bigger, it is usually called the application benchmark.

% Introduce benchmark suites
Benchmarks do not have the sole purpose of measuring the performance of an application.
Inversely they provide a way to measure the capabilities of some hardware.
For this reason, benchmark suites aggregate many application workloads, put them under a single harness, and simplify the performance evaluation process.
Examples of such suites are from Standard Performance Evaluation Corporation\footnote{\url{https://www.spec.org}} that has multiple suites, for instance, SPEC CPU for natively compiled benchmarks and SPECjbb, which is \mbox{JVM-based}.
Other standalone suites used in this thesis are Renaissance~\cite{prokopec2019renaissance}, DaCapo~\cite{blackburn2006dacapo}, and Scalabench~\cite{sewe2011capo}.
All suites mentioned above are \mbox{JVM-based} except for SPEC CPU, which has benchmarks written in C, C++, and Fortran and hence is natively compiled.
Apart from testing hardware performance, benchmark suites are good candidates for research in performance measurement methods as they strive to provide a wide range of real-world representative workloads.

% Reason about long execution time and attractiveness of the cloud approach
The requirement for multiple repetitions causes an inherent trade-off between accuracy and execution time.
Execution of a benchmark suite for some software project version to identify newly introduced performance regression can take an overwhelming 3000 machine hours per day as analyzed by~\citet{bulej2020duet} on an open-source GraalVM project~\cite{oracleGraal}.
By then, a new version could have come around.
This trade-off between measurement execution time and the number of commits between tested versions can be addressed from multiple angles.
One is to reduce the number of measurements by identification of commits that might introduce performance regression~\citet{oliveira2017perphecy}.
Other is to reduce the execution time of measurements by parallelization.
Effective parallelization involves maintaining multiple dedicated machines, which is difficult and costly.
Therefore, much research over the past decade has focused on offloading performance measurements to the cloud~\cite{leitner2016patterns, laaber2019software, abedi2017conducting}.

% Describe the nature of benchmarking in the cloud
\section{Benchmarking in the cloud}
Cloud offers practically infinite hardware scaling capabilities with on-demand access and a \mbox{pay-for-what-you-use} principle.
That makes it a tempting platform for fast and accessible performance measurements.
Cloud offers a large scale of options to run your code on from bare metal offerings\footnote{AWS Outposts \url{https://aws.amazon.com/outposts/}} to running small pieces of code as lambdas\footnote{AWS Lambda \url{https://aws.amazon.com/lambda/}}.
The most commonly examined and accessible method is an \mbox{Infrastructure-as-a-Service} (IaaS) cloud offering where computing resources are acquired and released as a service.

IaaS is typically in form of virtual machines or containers with attached virtual disks.
These instances come with different configuration options, such as the number of virtual CPUs, memory size, network throughput, and virtual disk.
This gives practitioners some control over what hardware and software platform is used to run their code compared to higher-level offerings such as Platform-as-a-Service (PaaS) and Software-as-a-Service (SaaS).

% Performance stability in the cloud -~\citet{laaber2019software}.
Studies such as~\citet{leitner2016patterns} and~\citet{laaber2019software} summarize the viability of measuring software performance on IaaS offerings from different providers.
They point out that measuring software performance on the public cloud is a moving target, with providers constantly changing their offerings, hardware, and processes.
A common concern is that a given instance type may not run on the same type of hardware and that the same physical hardware might host multiple colocated instances.
Hardware heterogeneity, shared and virtualized infrastructure further amplifies the variability of performance measurements in the cloud environment~\cite{leitner2016patterns} even to the point that practitioners, using state-of-the-art performance measurement and statistical methods, might not be able to detect $1000\%$ slowdown of some measured software as shown by~\citet{laaber2019software}.

% Introduce statistical methods, later on, mention repeatability, cite do we teach useful statistics?
With many repetitions and variability across the results, it is necessary to carefully process the results and use statistical methods to find out if a code change introduces a new performance regression.
The use of statistical methods has been extensively studied in~\citet{bulej2017stat} and further analyzed in~\citet{laaber2019software} specifically in the cloud context.
To quantify the variability of the distribution of results from a benchmark~\citet{laaber2019software} use the coefficient of variance, also called mean standard deviation.
To compare results of two software versions~\citet{bulej2017stat} use both methods of overlapping confidence intervals computed by bootstrap procedure and hypothesis testing with Wilcoxon rank-sum test and its variation Mann-Whiteney U-test as well as Welch's t-test.
\citet{bulej2017stat} also raise concerns about the validity of these methods when used for performance measurement data.
Concerns stem from the fact that performance measurements don't satisfy many common assumptions that statistical methods make, such as random independent and identically distributed samples and asymptotic normality of the data.
Moreover, performance data also tend to exhibit long-range dependencies between samples, have unknown distributions with long tails, and are not stationary in general, which further complicates statistical analysis~\cite{bulej2017stat}.
So far, there does not seem to be any "silver bullet" statistical method that would be a good fit for performance data, and thus, these techniques need to be used with caution as reasoned in multiple studies~\cite{leitner2016patterns, laaber2019software, bulej2017stat}.

% Measurement methods designed for volatile cloud environment
Unfavorable conditions for benchmarking in the cloud spiked interest in research that focuses on performance measurement methods that reduce measurement variability, increase accuracy and reduce false positives of regression detection in a cloud environment.
One such method and current best practice~\cite{laaber2019software} is called \emph{Randomized Multiple Interleaved Trials} (RMIT) introduced by~\citet{abedi2017conducting}.
Trial refers to a single run of benchmark, with multiple inside iterations, on a particular version of the software it measures the performance of.
To compare performance between multiple versions of some software, one has to run the same benchmark with each alternative.
RMIT states that these runs of these alternatives should be run (1) multiple times as discussed above and (2) repetitions of all alternatives should be randomized to minimize both partial and periodic outside interference.

Another method that aims to reduce variance is duet benchmarking, first introduced in~\citet{bulej2019initial} and further expanded on in~\citet{bulej2020duet}.
The next chapter examines the duet method and its new asynchronous variant in detail and poses research goals for this thesis.
