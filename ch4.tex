\chapter{Overview of Results Processing}
\label{chap:results}

With multiple repetitions and inherent variance of measurements statistical analysis is used to determine performance regression in an A/B run.
This chapter describes statistical methods we use to determine and compare measurement variance and regression detection of an A/B run using different benchmarking methods.
In particular, it adjusts the confidence interval test from \citet{bulej2020duet} to work with asynchronous duet.

\section{A/A runs}

Finding a software project that exhibits certain slowdown is difficult since regression is often tied to the hardware running the benchmark.
Instead, research into performance measurements employs A/A measurement technique~\cite[laaber2019software, bulej2020duet].
A/A measurement runs the same version and later labels runs with A and B labels --- as if there were 2 different versions.
Since the code is the same the performance should be the same as well.
Usability of a benchmarking method can thus be a result of its ability to correctly detect A/A measurements correctly.
In the context of duet runs A/B labeling is performed on concurrently running duet pairs.

\section{Data filtering}
\label{sec:data_filtering}

In our experiments, we removed the first half of all iterations per run of \mbox{JVM-based} benchmarks to avoid warm-up, same as in~\citet{bulej2020duet}.
We kept all measurements of SPEC CPU benchmarks since it is natively compiled.

\section{Accuracy analysis and performance regression tests}

To address the variance of different benchmarking methods, using containers, across different environments (\emph{RQ3}), we use various statistical methods.
First we compare the raw variance of measurements after data filtering using relative standard deviation.
Then we compute the confidence intervals of paired iteration score difference similar to \citet{bulej2020duet}, but it has to be adjusted for asynchronous duet.
That is basis for the confidence interval test, and it allows us to compare the relative confidence interval width across different methods.
Another regression detection method is \mbox{Mann-Whitney} \mbox{u-test} that compares iteration score distributions between pairs.
Last we use both regression tests to explore the minimal detectable slowdowns~\cite{laaber2019software} of sequential and duet methods.

\subsection{Relative standard deviation}
\label{sec:cv}

Relative standard deviation, also known as the coefficient of variation ($CV$), is one way to determine the variance of iteration times.
For measurement type $t \in T$, benchmark $b \in B$, runs $R^{t, b} \in M^e$:
\begin{align*}
CV^{t, b} &= \frac{std(R^{t, b})}{mean(R^{t, b})}.
\end{align*}

The $CV$ is a measure of how the results of different benchmarks vary relative to each other using different benchmarking methods.
However, the variance of duet methods cannot be determined just based on $CV$.

For instance, higher $CV$ for given benchmark duet measurements compared to its sequential measurements doesn't imply worse predictability of duet methods because of the impact symmetry argument.
Overall variance might be higher, but the pairwise comparison could be more telling.
On the other hand, lower $CV$ for duet methods suggests better robustness of duet methods.

\subsection{Confidence interval test}
\label{sec:ci_test}

To determine if pairs manifest performance change use the method of confidence intervals from~\citet{bulej2020duet}.
This method also provides a variance comparison across benchmarking methods based on relative confidence interval width.
Here, we describe the method and extend its use for the asynchronous duet.

First, runs and their iterations need to be paired.
\begin{description}
    \item[Sequential] runs don't have any natural pairing but are randomly paired by the nature of A/A runs labeling. We decided to pair their iterations sequentially
        $$
        is\_pair(i^A_i, i^B_j) = True \Longleftrightarrow i = j.
        $$
    \item[Synchronous duet] runs are naturally paired - sequentially by iterations, thus they have the same $is\_pair$ predicate.
    \item[Asynchronous duet] runs can't be paired sequentially.
        Considering A/B measurements, non-synchronized iterations will inevitably diverge, hence sequential iteration pairing wouldn't support impact symmetry.
        Instead, we decided to pair iterations $i^A$ and $i^B$ based on the minimum overlap ratio $m \in (0, 1)$ and thus define a set of filtered overlaps as
        \begin{equation}\label{eq:overlap_set_filtered}
        O^{e,b}_m = \{(i^A, i^B)~|~(i^A, i^B) \in O^{e, b}, overlap\_rate(i^A, i^B) > m\},
        \end{equation}
        where $O^{e, b}$ is set of overlaps from~\cref{eq:overlap_set}.
        The smaller the $m$, the fewer pairs of iterations there will be and vice versa.
        Formally, denote the predicate for asynchronous duet pairing with minimum overlap ratio parameter $m$ as
        $$
        is\_pair(i^A, i^B) = True \Longleftrightarrow (i^A, i^B) \in O^{e,b}_m.
        $$
        Analysis of how many overlaps there are for different values of $m$ is in~\cref{sec:rq2}.
\end{description}
Each pairing method returns a set of paired iterations for an experiment $M^{e, b}_t$,
$$
Pairs(M^{e, b}_t) = \{(i^A, i^B)~|~is\_pair(i^A, i^B),~\forall~i^A, i^B \in M^{e, b}_t\}.
$$

Second, paired iteration times are subtracted to determine the difference between all pairs which yields random sample $X$ with distribution $F_X$
$$
X = \{i^A - i^B~|~(i^A, i^B)~\in~Pairs(M^{e, b}_t)\}.
$$

Furthermore, \citet{bulej2017stat} recommend to use \emph{run means} sampling instead of \emph{runs merged} sampling.
The runs merged method uses all paired iterations from all runs as a sample.
Run means, on the other hand, aggregates paired iterations in a paired run using statistical methods, for example, arithmetic mean and uses those as samples.
Given $r$ runs each with $i$ iterations \emph{runs merged} results in $r * i$ observations and \emph{run means} results in $r$ observations.

We decided to use \emph{run means} sampling because we have the same amount of runs for each benchmark but not identical iteration count across suites.

These observations are used to create $95\%$ confidence interval using non-parametric bootstrap with $9999$ draws with replacement and arithmetic mean as statistics.\footnote{We used the bootstrap method from python \lstinline{scipy} package \url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html} that uses a \mbox{bias-corrected} and accelerated method to compute the CI by default.}
For A/A runs we expect distribution $F_X$ to be centered around~$0$.
Therefore, if the confidence interval encompasses~$0$, we consider the workloads equal, otherwise report a difference.

The confidence computation in~\citet{bulej2020duet} differs for synchronous duet because it uses difference of paired iteration durations together with arithmetic mean instead of iteration duration ratio with geometric mean.
This change simplifies the comparison process because the samples of different benchmarking methods have the same units.
 
\subsubsection{Variability comparison using CI}
\label{sec:ci_width}

Another use of these confidence intervals is to compute their relative width to compare the variability of different measurement methods similar to~\citet{bulej2020duet}.
\begin{equation}\label{eq:relative_ci_width} 
CI_{relative\_width}(X) = \frac{CI_{width}(X)}{mean(X)}.
\end{equation}

\subsection{Mann-Whitney u-test}
\label{sec:utest}

The next method to evaluate regression in an A/B run is to use hypothesis testing.
One such test used for purpose of A/B performance evaluation is Mann-Whitney \mbox{u-test}~\cite{bulej2017stat,laaber2019software}.
It is a non-parametric test of the null hypothesis that two samples come from the same distribution.
One sample is the iteration durations of pair A and the other is of pair B.

We used \mbox{u-test} implementation from \lstinline{scipy} python package~\footnote{\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html}} in its \mbox{two-sided} variant and decide to reject the null hypothesis if \mbox{p-value} is less than $5\%$.
Hence, if \mbox{p-value} is below $5\%$ for a given benchmark, test method, and environment reject the null hypothesis --- sample distributions are distinct enough, thus we report a performance change.

\section{Minimal detectable slowdown}
\label{sec:mds}

\Citet{laaber2019software} explore the minimal detectable slowdowns (MDS) for a sequential micro-benchmarking method to determine which micro-benchmarks are viable for cloud benchmarking.
We use this method to determine the viability of duet methods compared to sequential methods in the cloud environment.

First, A/A test results are adjusted to emulate a scenario where pair B is $s$ times wlog slower --- emulating A/B test results where version B has a performance regression.
For all iterations running version B, iteration runtime is multiplied by $s$.
Formally we adjust the experiment $M^{e}$ as
$$
M^{e}_{slowed} = \{(R^A, {i^B * s~|~\forall i^B \ in R^B})~|~\forall (R^A, R^B) \in M^{e}\}.
$$
Increased iteration times of pair B affect the overlap of A and B during asynchronous duet.
Therefore, recompute the start and end timestamps of each B's iteration, assuming the gaps between iterations remain unchanged.

Then the altered results can be compared as if they were A/B runs using the confidence interval test or \mbox{Mann-Whitney} \mbox{u-test} while the slowdown is known.
