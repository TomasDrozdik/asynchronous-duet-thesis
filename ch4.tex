\chapter{Overview of Results Processing}
\label{chap:results}

With multiple repetitions and inherent variance of measurements statistical analysis is used to determine performance regression in an A/B run.
This chapter describes statistical methods we use to determine and compare measurement variance and regression detection of an A/B run using different benchmarking methods.
In particular, it adjusts the confidence interval test from \citet{bulej2020duet} to work with asynchronous duet.

\section{A/A runs}

Finding a software project that exhibits certain slowdown is difficult since regression is often tied to the hardware running the benchmark.
Instead, research into performance measurements employs A/A measurement technique~\cite[laaber2019software, bulej2020duet].
A/A measurement runs the same version and later labels runs with A and B labels --- as if there were 2 different versions.
Since the code is the same the performance should be the same as well.
Usability of a benchmarking method can thus be a result of its ability to correctly detect A/A measurements correctly.
In the context of duet runs A/B labeling is performed on concurrently running duet pairs.

\section{Data filtering}
\label{sec:data_filtering}

In our experiments, we removed the first half of all iterations per run of \mbox{JVM-based} benchmarks to avoid warm-up, same as in~\citet{bulej2020duet}.
We kept all of our SPEC CPU benchmark iterations.

\section{Accuracy analysis and performance regression tests}

\xxx{TODO: this sentence feels like its repeating what has been said already.}
The goal of benchmarking is to determine performance speedup or a regression.
The high variance of iteration times makes this more difficult, but the synchronous duet method has shown variance improvements from $5.03 \times$ on average for Scalabench and DaCapo workloads to $37.4 \times$ for SPEC CPU~\cite{bulej2020duet}.
To address the variance of benchmarking methods, using containers, across different environments (\emph{RQ3}), we use various statistical methods.

\subsection{Relative standard deviation}
\label{sec:cv}

Relative standard deviation, also known as the coefficient of variation ($CV$), is one way to determine the variance of iteration times.
For measurement type $t \in T$, benchmark $b \in B$, runs $R^{t, b} \in M^e$:
\begin{align*}
CV^{t, b} &= \frac{std(R^{t, b})}{mean(R^{t, b})} = \frac{std(\{i~|~i \in R^{t, b}\})}{mean(\{i~|~i \in R^{t, b}\})}
\end{align*}

The $CV$ is a measure of how the results of different benchmarks vary relative to each other using different benchmarking methods.
However, the variance of duet methods cannot be determined just based on $CV$.

For instance, higher $CV$ for given benchmark's duet measurements compared to its sequential measurements doesn't imply worse predictability of duet methods because of the impact symmetry argument.
Overall variance might be higher, but the pairwise comparison could be more telling.
On the other hand, lower $CV$ for duet methods suggests better robustness of duet methods.

\subsection{Confidence interval test}
\label{sec:ci_test}

To determine if pairs manifest performance change use the method of confidence intervals from~\citet{bulej2020duet}.
This method also provides a variance comparison across benchmarking methods based on relative confidence interval width.
Here, we describe the method and extend its use for the asynchronous duet.

First, runs and their iterations need to be paired.
\begin{description}
    \item[Sequential] runs don't have any natural pairing but are randomly paired by the nature of A/A runs labeling. We decided to pair their iterations sequentially
        $$
        is\_pair(i^A_i, i^B_j) = i == j
        $$
    \item[Synchronous duet] runs are naturally paired - sequentially by iterations, thus they have the same $is\_pair$ predicate.
    \item[Asynchronous duet] runs can't be paired sequentially.
        Considering A/B measurements, non-synchronized iterations will inevitably diverge, hence sequential iteration pairing wouldn't support impact symmetry.
        Instead, we decided to pair iterations $i^A$ and $i^B$ based on the minimum overlap ratio $m \in (0, 1)$ and thus define a set of filtered overlaps as
        $$
        O^{e,b}_m = \{(i^A, i^B)~|~(i^A, i^B) \in O^{e, b}, overlap\_rate(i^A, i^B) > m\}.
        $$
        The smaller the $m$, the fewer pairs of iterations there will be and vice versa.
        It follows that this is a predicate for asynchronous duet pairing with minimum overlap ratio parameter $m$
        $$
        is\_pair(i^A, i^B) = (i^A, i^B) \in O^{e,b}_m.
        $$
        Analysis of how many overlaps there are for different $m$s is in~\cref{sec:rq2}.
\end{description}
Each pairing method returns a set of paired iterations for an experiment $M^{e, b}_t$,
$$
Pairs(M^{e, b}_t) = \{(i^A, i^B)~|~is\_pair(i^A, i^B),~\forall~i^A, i^B \in M^{e, b}_t\}.
$$

Second, paired iteration times are subtracted to determine the difference between all pairs which yields random sample $X$ with distribution $F_X$
$$
X = \{i^A - i^B~|~(i^A, i^B)~\in~Pairs(M^{e, b}_t)\}.
$$

Furthermore, \citet{bulej2017stat} recommend to use \emph{run means} sampling instead of \emph{runs merged} sampling.
The runs merged method uses all paired iterations from all runs as a sample.
Run means, on the other hand, aggregates paired iterations in a paired run using statistical methods, for example, arithmetic mean and uses those as samples.
Given $r$ runs each with $i$ iterations \emph{runs merged} results in $r * i$ observations and \emph{run means} results in $r$ observations.

These observations are used to create $95\%$ confidence interval using non-parametric bootstrap with $9999$ draws with replacement and arithmetic mean as statistics.\footnote{We used the bootstrap method from python \lstinline{scipy} package \url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html} that uses a \mbox{bias-corrected} and accelerated method to compute the CI by default.}
For A/A runs we expect distribution $F_X$ to be centered around~$0$.
Therefore, if the confidence interval encompasses~$0$, we consider the workloads equal, otherwise report a difference.
 
\subsubsection{Variability comparison using CI}
\label{sec:ci_width}

Another use of these confidence intervals is to compute their relative width and compare the variance of different measurement methods~\cite{bulej2019initial}.
$$
CI_{relative\_width}(X) = \frac{CI_{width}(X)}{mean(X)}.
$$

\subsection{Mann-Whitney u-test}
\label{sec:utest}

The next method to evaluate regression in an A/B run is to use hypothesis testing.
One such test used for purpose of A/B performance evaluation is Mann-Whitney \mbox{u-test}~\cite{bulej2017stat,laaber2019software}.
It is a non-parametric test of the null hypothesis that two samples come from the same distribution.
One sample is the iteration durations of pair A and the other is of pair B.

We used \mbox{u-test} implementation from \lstinline{scipy} python package~\footnote{\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html}} in its two-sided variant and decide to reject the null hypothesis if \mbox{p-value} is less than $5\%$.
Hence, if \mbox{p-value} is below $5\%$ for a given benchmark, test method, and environment reject the null hypothesis --- sample distributions are distinct enough, thus we report a performance change.

\section{Minimal detectable slowdown}
\label{sec:mds}

%Same as in~\citet{laaber2019software}, caveats with the overlaps.
\Citet{laaber2019software} explore the minimal detectable slowdowns (MDS) for a sequential micro-benchmarking method to determine which micro-benchmarks are viable for cloud benchmarking.
We use this method to determine the viability of duet methods compared to sequential methods in the cloud environment.

First, A/A test results need to be adjusted --- emulate a scenario where pair B is $s$ times wlog slower --- emulating A/B test results where version B has a performance regression.
For all iterations running version B, multiply its runtime by $s$.
$$
\forall (R^A, R^B) \in M^{e} ~:~ R^B_{slowed} = \{i * s~|~\forall i \in R^B \}.
$$\
For asynchronous duet increased iteration times of B affect the overlap of A and B.
Thus we need to recompute the start and end timestamps of each B's iteration, assuming the gaps in between workload runs remain unchanged.

Second, use either the confidence interval test or \mbox{Mann-Whitney} \mbox{u-test} to determine if adjusted A/B measurements show performance regression on different known slowdowns.
