\chapter{Evaluation}
\label{chap:evaluation}

Describe what has been measured and where.


\section{Measurement Setup}

\subsection{Selected benchmarks}

What benchmarks are in the study and why?

\begin{enumerate}
  \item Renaissance suite
  \item ScalaBench/DeCappo suite
  \item some non JVM bench suite - CoreMark / SpecCPU
\end{enumerate}

\subsubsection{Reasoning on how and why those benchmarks were picked}

Description of what they do. Either reference them and describe few that we will pick later.

Metrics:
\begin{enumerate}
  \item CPU - user/sys, pipeline utilization
  \item Memory
  \item Interractions with FS
  \item Rate of parallelism, lock contention
  \item JVM specific stats
        \begin{enumerate}
          \item TODO
        \end{enumerate}
\end{enumerate}

Benchmark profiling?

\subsection{Selected platforms}

Describe what platforms were picked, why and what was their configuration.
\begin{enumerate}
  \item Dedicated server
  \item On-prem private cloud - DigitalOcean
  \item AWS - different configurations
  \item Personal laptop? - to illustrate usability in general developer workflow
\end{enumerate}

\subsection{Run summary}

Number of benchmarks run in different configurations as well as different methods used - asynchronous/synchronous duet, sequential.


\section{Measurements}

Address each research question:

\subsection{RQ1: What is the variance of asynchronous-duet compared to synchronous duet and sequential execution?}

\subsection{RQ2: How does choice of environment affect the variance?}

\subsection{RQ3: What are the effects of duet benchmarking on run time and costs?}

\subsection{RQ4: What categories of benchmarks and statistical methods, are viable for asynchronous-duet method. (stretch goal?)}
